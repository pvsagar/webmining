{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 1</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze the frequency of words in a string ## (5 points)\n",
    " - Define a function named \"**count_token**\" which (0.5 point)\n",
    "     * has a string as an input (0.5 point)\n",
    "     * splits the string into a list of tokens by space. For example, \"it's hello world\" will be split into two tokens [\"it's\", \"hello\",\"world!\"] (1 point)\n",
    "     * removes all spaces around each token (including tabs, newline characters (\"\\n\")) (0.5 point)\n",
    "     * removes empty tokens, i.e. *len*(token)==0 (0.5 point)\n",
    "     * converts all tokens into lower case (0.5 point)\n",
    "     * create a dictionary containing the count of every unique token, e.g. {'its': 5, 'hello':1,...} (1 point)\n",
    "     * returns the dictionary as the output (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define a class to analyze a document ## (5 points)\n",
    " - Define a new class called \"**Text_Analyzer**\" which does the following : (0.5 point)\n",
    "    - has two attributes: \n",
    "        * **input_string**, which receives the string value passed by users when creating an object of this class. (0.5 point)\n",
    "        * **token_count**, which is set to {} when an object of this class is created. (0.5 point)\n",
    "        \n",
    "    - a function named \"**analyze**\" that does the following: \n",
    "      * calls the function \"count_token\" to get a token-count dictionary. (1 point)\n",
    "      * saves this dictionary to the token_count attribute (0.5 point)\n",
    "      \n",
    "    - another function named \"**save_to_file**\", which \n",
    "      * has a string parameter which specifies the full name path of a file to be created (0.5 point)\n",
    "      * saves count_token dictionary into this file with each key-value pair as a line delimited by comma (see \"foo.csv\" in Exercise 10.3 for examples). (1.5 point)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. (Bonus) Segment documents by punctuation ## (4 points)\n",
    " - Create a new function called \"**corpus_analyze**\" which does the following :\n",
    "     * takes **a list of strings** as an input\n",
    "     * for each string, do the following:\n",
    "         * splits the string into a list of tokens by **any space** or **any punctuation** (i.e. any character from the list <font color=\"blue\">!\"#$%&'()\\*+,-./:;<=>?@[\\\\]^_`{|}~</font> ), e.g. \"it's hello world!\" should be split into a list [\"it\", \"s\", \"hello\", \"world\"] (2 points)\n",
    "         * removes leading and trailing spaces of each token \n",
    "         * removes any empty token or token with only 1 character\n",
    "         * converts all tokens into lower case \n",
    "     * creates a token count dictionary named **token_freq**, which gives the **total count** of each unique token in all the input strings, e.g. {'the', 100, 'of': 50, ...} (1 point)\n",
    "     * creates another dictionary called **token_to_doc**, where each key is a unique token, and the corresponding value is the list of indexes of the input strings that contain the token. For example {'the': [ 2, 5 ], 'of':[3, 4], ...}, i.e. the 2rd and 6th strings contain the token \"the\", and the 4th and 5th strings have token \"of\". (1 point)\n",
    "     * returns (token_freq, token_to_doc) as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'world': 1, '!': 1, 'world!': 1, 'a': 1, \"it's\": 1, 'example': 1, 'hello': 2, 'is': 1}\n",
      "{'hello': 2, 'world': 2, 'it': 1, 'is': 1, 'example': 1}\n",
      "{'hello': [0, 1], 'world': [0, 1], 'it': [1], 'is': [1], 'example': [1]}\n"
     ]
    }
   ],
   "source": [
    "# Structure of your solution to Assignment 1 \n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def count_token(text):\n",
    "\n",
    "    tokens=text.split(\" \")\n",
    "    tokens=[token.lower().strip() for token in tokens if len(token.strip())>0]\n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "\n",
    "    return token_count\n",
    "\n",
    "class Text_Analyzer(object):\n",
    "    \n",
    "    def __init__(self, doc):\n",
    "        \n",
    "        self.input_string=doc\n",
    "        self.token_count ={}\n",
    "          \n",
    "    def analyze(self):\n",
    "        self.token_count = count_token(self.input_string)\n",
    "        \n",
    "    def save_to_file(self, output_filepath):\n",
    "\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            writer=csv.writer(f, delimiter=\",\")\n",
    "            items=self.token_count.items()\n",
    "            writer.writerows(items)\n",
    "\n",
    "def corpus_analyze(docs):\n",
    "    \n",
    "    token_freq, token_to_doc = {}, {}\n",
    "    \n",
    "    # if PunktWordTokenizer from NLTK is used, it's also OK\n",
    "    \n",
    "    for doc_id,doc in enumerate(docs):\n",
    "        char_list = list(doc)\n",
    "        for idx, w in enumerate(char_list):\n",
    "            if w in \"!\\\"#$%&'()\\*+,-./:;<=>?@[\\\\]^_`{|}~\":\n",
    "                char_list[idx]=' '\n",
    "                \n",
    "        tokens=''.join(char_list).lower().split(' ')\n",
    "        \n",
    "        for token in tokens:\n",
    "            if len(token)>1:\n",
    "                if token in token_freq:\n",
    "                    token_freq[token]+=1\n",
    "                else:\n",
    "                    token_freq[token]=1\n",
    "\n",
    "                if token in token_to_doc:\n",
    "                    token_to_doc[token].append(doc_id)\n",
    "                else:\n",
    "                    token_to_doc[token]=[doc_id]\n",
    "                \n",
    "    return token_freq, token_to_doc                                                                            \n",
    "\n",
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    text='''Hello world!\n",
    "        It's is a hello world example !'''   \n",
    "    print(\"Test Q1:\\n\",count_token(text))\n",
    "    \n",
    "    # # The output of your text should be: \n",
    "    # {'world': 1, '!': 1, 'world!': 1, 'a': 1, \"it's\": 1, \n",
    "    # 'example': 1, 'hello': 2, 'is': 1}\n",
    "    \n",
    "    # Test Question 2\n",
    "    analyzer=Text_Analyzer(text)\n",
    "    analyzer.analyze()\n",
    "    analyzer.save_to_file(\"/Users/rliu/temp/test.csv\")\n",
    "    # You should be able to find the csv file with 8 lines, 2 columns\n",
    "    \n",
    "    #3 Test Question 3\n",
    "    docs=['Hello world!', \"It's is a hello world example !\"]\n",
    "    word_freq, token_to_doc=corpus_analyze(docs)\n",
    "    \n",
    "    print(\"Test Q3:\\n\", word_freq)\n",
    "    # output should be {'hello': 2, 'world': 2, 'it': 1, 'is': 1, 'example': 1}\n",
    "\n",
    "    print(token_to_doc)\n",
    "    # output should be {'hello': [0, 1], 'world': [0, 1], 'it': [1], 'is': [1], 'example': [1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
