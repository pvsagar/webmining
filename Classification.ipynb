{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Supervised Learning - Text Classification</center>\n",
    "References:\n",
    "* http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Finally, we come to machine learning ...\n",
    "What can the players do when their every move is studied and predicted ?\n",
    "\n",
    "<img src=\"machine_learning_cartoon.png\" width=\"60%\">\n",
    "https://www.kdnuggets.com/2018/06/cartoon-fifa-world-cup-football-machine-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review basic concepts of machine learning\n",
    "  * Cross validation\n",
    "  * Performance metrics: recall and precision\n",
    "* Text Classification  \n",
    "  * Assign a document into one  or more pre-defined categories (or labels)\n",
    "    * Input: \n",
    "      - a document $d$ \n",
    "      - a fixed set of classes C = {$c_1$, $c_2$,..., $c_J$}\n",
    "      - A training set of $m$ hand-labeled documents ($d_1,c_1$),....,($d_m,c_m$)\n",
    "    * Output: a classifier that predicts $d$ to some classes $c$ $\\subset$ C\n",
    "  * **Single-label** classification: e.g. spam dection, sentiment detection\n",
    "  * **Multi-label** classification: e.g. news categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review basic concepts of machine learning\n",
    "### 2.1. Model assessment and selection - How valid is a model? \n",
    "- Generalization: the prediction capability of a model ($f$) on independent test data, \n",
    "  - Given testing samples ($X, Y$), and prediction ($X, f(X)$)\n",
    "  - Testing error: $L(Y, f(X))$, e.g.\n",
    "     - squared error\n",
    "     - absolute error\n",
    "  \n",
    "- Data-rich situation: split data into training, validation, and test sets (e.g. 50%, 25%, 25%)\n",
    "  - training set: fit the model\n",
    "  - validation set: estimate prediction error for model selection\n",
    "  - test set: assess the prediction erorr of the final chosen model\n",
    "  <img src=\"train_validation_test.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Cross Validation\n",
    "- However, labeled data is always scarce. We cannot afford to set aside a validation set\n",
    "- $K$-fold cross validation: \n",
    "    - Data is separated into k subsets. Each time, one of the subsets is held as the test set (a.k.a holdout) and the rest of them is used as the training set. \n",
    "    <img src=\"cross_validation.png\" width=\"40%\"> [source] (http://spark-public.s3.amazonaws.com/nlp/slides/sentiment.pptx)\n",
    "    - This method repeats *k* times and each time with a different subset as the test set. \n",
    "    - Calculate average prediction error ($CV$) on K test sets $$ CV(\\alpha) = \\frac{1}{N} \\sum_{i=1}^{N}{L(y_i, f^{k(i)}(x_i, \\alpha))}$$ where $\\alpha$: the model parameters (e.g. the number of neighbours in $k$-NN), $f^{k(i)}$: the model fitted on the $k$th iteration, $N$: number of samples\n",
    "    - Tune model parameters ($\\alpha$) to minize the average prediction error\n",
    "    - Select the model with the minimal prediction error (along with $\\alpha$ determined)\n",
    "    - Fit the selected model to all the data\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Performance metrics\n",
    "  * Precision: precentage of true cases among the predicated true cases\n",
    "  * Recall:  precentage of true cases that have been retrieved over the total number of true cases\n",
    "  * F-score: $$\\frac{2*precision*recall}{precision+recall}$$\n",
    "  * Example: \n",
    "Confusion Matrix: <img src=\"confusion_matrix.png\">\n",
    "    * For \"YES\" group: \n",
    "      - precision=?, \n",
    "      - recall=?, \n",
    "      - f-score=?\n",
    "      <img src=\"precision_recall.png\" width=\"60%\">\n",
    "    * For \"NO\" group:\n",
    "      - precision=?, \n",
    "      - recall=?, \n",
    "      - f-score=?\n",
    "  * Overall model performance\n",
    "    * precision_macro (or recall_macro or f1_macro) is calculated as:\n",
    "      1. calculate precision for each label\n",
    "      2. average over labels \n",
    "    * precision_micro (or recall_micro or f1_micro): calculates metrics globally regardless of labels\n",
    "    * With inbalanced classes, the difference between these two metrics may be significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* Basic process\n",
    "  1. Load and preprocess sample data\n",
    "  2. Extract features: e.g. bag of words with TF-IDF weights\n",
    "  3. Split feature space into trainning and test sets following cross validation method\n",
    "  4. Train a classifier/model with the training dataset using selected classification algorithm for each fold\n",
    "  5. Calculate performance\n",
    " \n",
    "* Considerations for deciding text classification algorithms\n",
    "  - should be effective in high dimensional spaces (**curse of dimensionality**)\n",
    "  - should be effective even if **the number of features is greater than the number of samples**\n",
    "    * Is regression a good alogorithm if you have a small number of text samples?\n",
    "  - some good algorithms to start with:\n",
    "      - Naive Bayes (https://web.stanford.edu/class/cs124/lec/naivebayes.pdf): baseline for performance benchmarking of text classification algorithms\n",
    "      - Support Vector Machine (SVM). References:\n",
    "        - https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/\n",
    "        - http://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier)\\nSubj...</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text                   label\n",
       "0  From: sd345@city.ac.uk (Michael Collier)\\nSubj...           comp.graphics\n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...           comp.graphics\n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...  soc.religion.christian\n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...  soc.religion.christian\n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly...  soc.religion.christian"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.1.: Load data \n",
    "# Load datasets (http://qwone.com/~jason/20Newsgroups/)\n",
    "# For convenience, a subset of the data has been saved into \"twenty_news_data.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"twenty_news_data.csv\",header=0)\n",
    "data.head()\n",
    "\n",
    "type(data)\n",
    "\n",
    "# print out the full text of the first sample\n",
    "print(data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. TF-IDF matrix generation\n",
    "- Function: **sklearn.feature_extraction.text.TfidfVectorizer**(input='content',encoding='utf-8', decode_error='strict', token_pattern='(?u)\\b\\w\\w+\\b', lowercase=True, stop_words=None, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, norm='l2', use_idf=True, smooth_idf=True, ...)\n",
    "- Some useful parameters:\n",
    "    * **input** : string {'filename', 'file', 'content').\n",
    "    * **encoding** : encoding scheme, 'utf-8' by default.\n",
    "If bytes or files are given to analyze, this encoding scheme is used to decode.\n",
    "    * **decode_error** : {'strict', 'ignore', 'replace'}: Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are ‘ignore’ and ‘replace’.\n",
    "    * **token_pattern** : Regular expression denoting what constitutes a “token”. The default is '(?u)\\b\\w\\w+\\b', i.e. a token contains at least two word characters in unicode (note: ?u: unicode, \\b: space or non-word character, i.e. boundary, \\w: word character). \n",
    "    * **ngram_range** : tuple (min_n, max_n): The lower and upper boundary of the range of n-values for different n-grams to be extracted. \n",
    "    * **stop_words** : string {‘english’}, list, or None (default)\n",
    "    * **lowercase** : boolean, default True: Convert all characters to lowercase before tokenizing.\n",
    "    * **max_df/min_df** : float in range [0.0, 1.0] or int, default=1.0: When building the vocabulary ignore terms that have a document frequency strictly higher (lower) than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. \n",
    "    * **max_features** : int or None, default=None. If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "    * **norm** : 'l1', 'l2' or None, optional. Norm used to normalize term vectors. None for no normalization.\n",
    "    * **use_idf** : boolean, default=True. Enable inverse-document-frequency reweighting.\n",
    "    * **smooth_idf** : boolean, default=True. Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions.\n",
    "- For all the parameters, see http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of dtm: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "size of tfidf matrix: (2257, 35788)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2 Create TF-IDF Matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# initialize the TfidfVectorizer \n",
    "\n",
    "tfidf_vect = TfidfVectorizer() \n",
    "\n",
    "# with stop words removed\n",
    "# tfidf_vect = TfidfVectorizer(stop_words=\"english\") \n",
    "\n",
    "# generate tfidf matrix\n",
    "dtm= tfidf_vect.fit_transform(data[\"text\"])\n",
    "\n",
    "print(\"type of dtm:\", type(dtm))\n",
    "print(\"size of tfidf matrix:\", dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of words: 35788\n",
      "type of vocabulary: <class 'dict'>\n",
      "index of word 'city' in vocabulary: 8696\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3. Examine TF-IDF\n",
    "\n",
    "# 1. Check vocabulary\n",
    "\n",
    "# Vocabulary is a dictionary mapping a word to an index\n",
    "\n",
    "# the number of words in the vocabulary\n",
    "print(\"total number of words:\", len(tfidf_vect.vocabulary_))\n",
    "\n",
    "print(\"type of vocabulary:\", \\\n",
    "      type(tfidf_vect.vocabulary_))\n",
    "print(\"index of word 'city' in vocabulary:\", \\\n",
    "      tfidf_vect.vocabulary_['city'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: \n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n",
      "\n",
      "tfidf weights: \n",
      "\n",
      "(35788,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('collier', 0.3841803935867984),\n",
       " ('city', 0.314400065528974),\n",
       " ('071', 0.25612026239119895),\n",
       " ('laserjet', 0.24645540709354397),\n",
       " ('477', 0.24645540709354397),\n",
       " ('converting', 0.21567205914741705),\n",
       " ('michael', 0.1962279892331408),\n",
       " ('iii', 0.18626015109199115),\n",
       " ('hp', 0.17358472047671197),\n",
       " ('files', 0.13635772403701527),\n",
       " ('sd345', 0.1348710554299733),\n",
       " ('8565', 0.1348710554299733),\n",
       " ('ec1v', 0.1348710554299733),\n",
       " ('x3769', 0.1348710554299733),\n",
       " ('0hb', 0.1348710554299733),\n",
       " ('tif', 0.12806013119559947),\n",
       " ('email', 0.125601499991304),\n",
       " ('ac', 0.12491817585060791),\n",
       " ('hpgl', 0.12322770354677198),\n",
       " ('img', 0.12322770354677198)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.4 check words with top tf-idf wights in a document, \n",
    "# e.g. 1st document\n",
    "\n",
    "# get mapping from word index to word\n",
    "# i.e. reversal mapping of tfidf_vect.vocabulary_\n",
    "voc_lookup={tfidf_vect.vocabulary_[word]:word \\\n",
    "            for word in tfidf_vect.vocabulary_}\n",
    "\n",
    "print(\"\\nOriginal text: \\n\"+data[\"text\"][0])\n",
    "\n",
    "print(\"\\ntfidf weights: \\n\")\n",
    "\n",
    "# first, covert the sparse matrix row to a dense array\n",
    "doc0=dtm[0].toarray()[0]\n",
    "print(doc0.shape)\n",
    "\n",
    "# get index of top 20 words\n",
    "top_words=(doc0.argsort())[::-1][0:20]\n",
    "[(voc_lookup[i], doc0[i]) for i in top_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "labels:  ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "precision:  [1.         0.9702381  0.96969697 0.78059072]\n",
      "recall:  [0.73972603 0.94767442 0.93023256 0.98404255]\n",
      "f-score:  [0.8503937  0.95882353 0.9495549  0.87058824]\n",
      "support:  [146 172 172 188]\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       1.00      0.74      0.85       146\n",
      "         comp.graphics       0.97      0.95      0.96       172\n",
      "               sci.med       0.97      0.93      0.95       172\n",
      "soc.religion.christian       0.78      0.98      0.87       188\n",
      "\n",
      "           avg / total       0.92      0.91      0.91       678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5. classification using a single fold\n",
    "\n",
    "# use MultinomialNB algorithm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# import method for split train/test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import method to calculate metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\\\n",
    "                dtm, data[\"label\"], test_size=0.3, random_state=0)\n",
    "\n",
    "# print(dtm)\n",
    "# print(\"X train\",X_train)\n",
    "# print(\"X test\",X_test)\n",
    "# print(\"Y train\",y_train)\n",
    "# print(\"Y test\",y_test)\n",
    "\n",
    "print(type(y_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train a multinomial naive Bayes model using the testing data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "# predict the news group for the test dataset\n",
    "predicted=clf.predict(X_test)\n",
    "\n",
    "# get the list of unique labels\n",
    "labels=sorted(data[\"label\"].unique())\n",
    "\n",
    "# calculate performance metrics. \n",
    "# Support is the number of occurrences of each label\n",
    "\n",
    "precision, recall, fscore, support=\\\n",
    "     precision_recall_fscore_support(\\\n",
    "     y_test, predicted, labels=labels)\n",
    "\n",
    "print(\"labels: \", labels)\n",
    "print(\"precision: \", precision)\n",
    "print(\"recall: \", recall)\n",
    "print(\"f-score: \", fscore)\n",
    "print(\"support: \", support)\n",
    "\n",
    "# another way to get all performance metrics\n",
    "print(classification_report\\\n",
    "      (y_test, predicted, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 35788)\n",
      "\n",
      " God is love\n",
      "alt.atheism: 0.171\n",
      "comp.graphics: 0.044\n",
      "sci.med: 0.053\n",
      "soc.religion.christian: 0.732\n",
      "'God is love' => soc.religion.christian\n",
      "\n",
      " OpenGL on the GPU is fast\n",
      "alt.atheism: 0.174\n",
      "comp.graphics: 0.367\n",
      "sci.med: 0.234\n",
      "soc.religion.christian: 0.224\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.6.  predict new documents\n",
    "\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# generate tifid for new documents\n",
    "X_new_tfidf = tfidf_vect.transform(docs_new)\n",
    "\n",
    "print(X_new_tfidf.shape)\n",
    "\n",
    "# predict probability that each document belongs to a class\n",
    "predicted_p = clf.predict_proba(X_new_tfidf)\n",
    "\n",
    "# predict classes for new documents\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for idx, doc in enumerate(docs_new):\n",
    "    print('\\n', doc)\n",
    "    for j, label in enumerate(labels):\n",
    "        print('% s: %.3f'%(labels[j], predicted_p[idx][j]))\n",
    "    print('%r => %s' % (doc, predicted[idx]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.7. Classification with stop words removed\n",
    "# Can removing stop words improves performance?\n",
    "# In Exercise 3.2, uncomment line 10 and comment line 7\n",
    "# Run Exercise 3.2, 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-edc7f914b855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#clf = MultinomialNB(alpha=0.5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m                     \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test data set average precision:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_precision_macro'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "source": [
    "# Exercise 3.8. Run 5-fold cross validation\n",
    "# to show the generalizability of the model\n",
    "\n",
    "# import cross validation method\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \\\n",
    "           \"f1_macro\"]\n",
    "\n",
    "clf = MultinomialNB()\n",
    "#clf = MultinomialNB(alpha=0.5)\n",
    "\n",
    "cv = cross_validate(clf, dtm, data[\"label\"], \\\n",
    "                    scoring=metrics, cv=5)\n",
    "print(\"Test data set average precision:\")\n",
    "print(cv['test_precision_macro'])\n",
    "print(\"\\nTest data set average recall:\")\n",
    "print(cv['test_recall_macro'])\n",
    "print(\"\\nTest data set average fscore:\")\n",
    "print(cv['test_f1_macro'])\n",
    "\n",
    "# To see the performance of training data set use \n",
    "# cv['train_xx_macro']\n",
    "print(\"\\ntraining data average f1:\\n\", cv['train_f1_macro'])\n",
    "\n",
    "# The metrics are quite stable across folds.\n",
    "# The performance between training and test sets is small\n",
    "# This indicates the model has good generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.9. Multinominal NB \n",
    "# with different smoothing parameter alpha\n",
    "# comment line 11 and uncomment 12 in Exercise 3.8\n",
    "# use different alpha value to see if it affects performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.10393167, 0.09794426]), 'score_time': array([0.01998925, 0.01998758]), 'test_precision_macro': array([0.96284554, 0.95543853]), 'train_precision_macro': array([0.99916667, 1.        ]), 'test_recall_macro': array([0.96006408, 0.95092921]), 'train_recall_macro': array([0.99895833, 1.        ]), 'test_f1_macro': array([0.96105757, 0.95207155]), 'train_f1_macro': array([0.99906072, 1.        ])}\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.10. SVM model\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "#from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn import svm\n",
    "\n",
    "metrics = ['precision_macro', 'recall_macro', \"f1_macro\"]\n",
    "\n",
    "# initiate an linear SVM model\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "cv = cross_validate(clf, dtm, data[\"label\"], \\\n",
    "                    scoring=metrics, cv=2)\n",
    "\n",
    "print(cv)\n",
    "\n",
    "# print(\"Test data set average precision:\")\n",
    "# print(cv['test_precision_macro'])\n",
    "# print(\"\\nTest data set average recall:\")\n",
    "# print(cv['test_recall_macro'])\n",
    "# print(\"\\nTest data set average fscore:\")\n",
    "# print(cv['test_f1_macro'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Parameter tuning using grid search\n",
    "* Each classification model has a few parameters\n",
    "  * e.g. \"stop_words\": \"english\" or None, min_df: [1,2,3, ...]\n",
    "  * e.g. MultinomialNB(alpha=1.0)\n",
    "  * e.g. LinearSVC(C=1.0, penalty=’l2’, loss=’squared_hinge’,...)\n",
    "* Instead of tweaking the parameters of the various components, it is possible to run an exhaustive search of the best parameters on a grid of possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.3.1 Grid search\n",
    "\n",
    "# import pipeline class\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# build a pipeline which does two steps all together:\n",
    "# (1) generate tfidf, and (2) train classifier\n",
    "# each step is named, i.e. \"tfidf\", \"clf\"\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', MultinomialNB())\n",
    "                   ])\n",
    "\n",
    "# set the range of parameters to be tuned\n",
    "# each parameter is defined as \n",
    "# <step name>__<parameter name in step>\n",
    "# e.g. min_df is a parameter of TfidfVectorizer()\n",
    "# \"tfidf\" is the name for TfidfVectorizer()\n",
    "# therefore, 'tfidf__min_df' is the parameter in grid search\n",
    "\n",
    "parameters = {'tfidf__min_df':[1, 2,5,10],\n",
    "              'tfidf__stop_words':[None,\"english\"],\n",
    "              'clf__alpha': [0.5,1.0,2.0],\n",
    "}\n",
    "\n",
    "# the metric used to select the best parameters\n",
    "metric =  \"f1_macro\"\n",
    "\n",
    "# GridSearch also uses cross validation\n",
    "gs_clf = GridSearchCV\\\n",
    "(text_clf, param_grid=parameters, \\\n",
    " scoring=metric, cv=5)\n",
    "\n",
    "# due to data volume and large parameter combinations\n",
    "# it may take long time to search for optimal parameter combination\n",
    "# you can use a subset of data to test\n",
    "gs_clf = gs_clf.fit(data[\"text\"], data[\"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf__alpha :  0.5\n",
      "tfidf__min_df :  2\n",
      "tfidf__stop_words :  english\n",
      "best f1 score: 0.9684663644232789\n"
     ]
    }
   ],
   "source": [
    "# gs_clf.best_params_ returns a dictionary \n",
    "# with parameter and its best value as an entry\n",
    "\n",
    "for param_name in gs_clf.best_params_:\n",
    "    print(param_name,\": \",gs_clf.best_params_[param_name])\n",
    "\n",
    "print(\"best f1 score:\", gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 3.3.2 Grid search\n",
    "# Modify Exercise 3.3 and Exercise 3.8 \n",
    "# to use the best parameters found\n",
    "# re-create the Multinominal NB classifier\n",
    "\n",
    "# also, check the dimension reduction of feature space by set min_df to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-label classification\n",
    "- So far we only cover single-label classification, i.e. assign one class to each sample\n",
    "- Multilabel classification emerges as a challenging problem, where classes are not mutually exclusive \n",
    "  * music categorization \n",
    "  * semantic classification of images\n",
    "  * tagging\n",
    "- **One-Vs-the-Rest** Strategy (a.k.a **one-vs-all**)\n",
    "  * fitting one classifier per class. For each classifier, the class is fitted against all the other classes.\n",
    "  * for $n$ classes (labels), $n$ classifier is needed\n",
    "  * Advantage: good interpretability - Since each class is represented by one and only one classifier, it is possible to gain knowledge about the class by inspecting its corresponding classifier\n",
    "  * Disadvantage: \n",
    "     * many classifiers are created if there is a large number classes\n",
    "     * ignore the structure (or dependencies) of classes\n",
    "- **Class indication matrix** (or **one-hot encoding**): Encode categorical integer features using a one-hot aka one-of-K scheme. \n",
    "\n",
    "| Document    | Money       | Investment | Crime & Justice |\n",
    "| :-----------|:-----------:|:----------:|:--------------:|\n",
    "| 1           | 0           |      0     | 1              |\n",
    "| 2           | 1           |      1     | 0              |\n",
    "| 3           | 1           |      0     | 0              |\n",
    "| 4           | 0           |      1     | 1              |\n",
    "\n",
    "- **dataset**: Yahoo News Ranked Multilabel Learning dataset (http://research.yahoo.com)\n",
    "  - A subset is selected\n",
    "  - 4 classes, 6426 samples\n",
    "  \n",
    "- **Discussion**: can you apply Naive Bayes for multi-label classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Multi-label classification- Load data\n",
    "\n",
    "import json\n",
    "data=json.load(open(\"../../dataset/ydata.json\",\"r\"))\n",
    "\n",
    "docs,labels=zip(*data)\n",
    "\n",
    "# show sample examples\n",
    "docs[1]\n",
    "labels[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2 One-hot coding of classes\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y=mlb.fit_transform(labels)\n",
    "# check size of indicator matrix\n",
    "Y.shape\n",
    "# check classes\n",
    "mlb.classes_\n",
    "\n",
    "# check # of samples in each class\n",
    "np.sum(Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4.3 Multi-label classification- one vs. rest classifier\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# split dataset into train (70%) and test sets (30%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\\\n",
    "                docs, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "classifier = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=\"english\",\\\n",
    "                              min_df=2)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4 Multi-label classification- Performance report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "predicted.shape\n",
    "predicted[0:2]\n",
    "Y_test[0:2]\n",
    "\n",
    "print(classification_report\\\n",
    "      (Y_test, predicted, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Encoding and Decoding\n",
    "https://www.agiliq.com/blog/2014/11/character-encoding-and-unicode/\n",
    "https://www.agiliq.com/blog/2014/12/understanding-python-unicode-str-unicodeencodeerro/\n",
    "\n",
    "- Computers only work with binary (0 or 1). Any character needs to have **a binary representation** so computer can store it on disk or in the memory. However, there are various ways in which characters can be converted to binary.\n",
    "\n",
    "- **Unicode** provides standard code points for different characters. It can give code point for any character in any language.\n",
    "  - e.g. 'a' <-> integer 97, hexadecimal 61 (denoted as '\\u0061' or '\\x61')\n",
    "  - e.g. 'ä' <-> integer 228, hexadecimal E4\n",
    "  \n",
    "- Python 3 always stores **text strings as sequences of Unicode code points**.\n",
    "- However, in Python 2, text strings are stored as **binary representations** (i.e. bytes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Encoding\n",
    "- **Encoding** means the process of converting a string to a binary representation. \n",
    "  - There are diffent **coding schemes**  \n",
    "      - **ascii**: encodes 128 specified characters into seven-bit integers \n",
    "        - e.g. a <-> 01100001 \n",
    "      - **utf-8**: use one to four 8-bit bytes to encode 1,112,064 characters\n",
    "        - ä <-> 11000011 10100100, or '\\xc3\\xa4' (hexadecimal c3a4)    \n",
    "      - **latin-1**: map codepoints to byte values directly\n",
    "        - ä <-> '\\xe4' (hexadecimal 00E4)\n",
    "  - Each encoding, which confirms to Unicode, has **a one-to-one mapping between a Unicode code point and the binary representation of codepoint**.\n",
    "  \n",
    "- Function **encode()**: convert a Unicode string to a binary representation according to an encoding scheme\n",
    "- **UnicodeEncodeError**: Encode a unicode string which is not in the scope of encoding scheme\n",
    "  - e.g. try to encode u'\\u00E4' with ascii scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.1\n",
    "\n",
    "s =  u'\\xE4'  # set unicode string. Note prefix u\n",
    "\n",
    "# encode into binary\n",
    "utf_s=s.encode(\"utf-8\")\n",
    "print(utf_s)\n",
    "\n",
    "# During printing or writing files, \n",
    "# since Python can only print ‘str’ (binary bytes)\n",
    "# it converts the ‘unicode’ into ‘str’ \n",
    "# using default system encoding\n",
    "print(s)\n",
    "\n",
    "# to check default encoding scheme\n",
    "import sys\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.2 UnicodeEncodeError\n",
    "\n",
    "s =  u'this is a strange \\xE4 character'\n",
    "\n",
    "# However, you cannot encode s using ascii, why?\n",
    "utf_s=s.encode(\"ascii\")\n",
    "print(utf_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Decoding\n",
    "- **decoding**: the process of converting an encoded binary representation into Unicode codepoint.\n",
    "\n",
    "- Function **decode()**: convert a binary string to a Unicode string according to an encoding scheme\n",
    "- **UnicodeDecodeError**: decode a binary string which is not in the scope of encoding scheme\n",
    "  - e.g. try to decode b'\\u00E4' with ascii scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.1. Decoding with UTF-8\n",
    "\n",
    "# A binary string (i.e. byte) has a prefix \"b\"\n",
    "s =  b'\\xc3\\xa4'\n",
    "utf_s = s.decode('utf-8') # convert to Unicode using UTf-8. The result is u'\\xE4'\n",
    "\n",
    "print(utf_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.2. UnicodeDecodeError\n",
    "\n",
    "s =  b'\\xc3\\xa4'\n",
    "utf_s = s.decode('ascii') # convert to Unicode using ascii\n",
    "print(utf_s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.2. Encoding/decoding exception handling\n",
    "# 'strict', 'ignore', and 'replace' \n",
    "\n",
    "s =  b'strange \\xc3\\xa4 text'\n",
    "utf_s = s.decode('ascii', errors='ignore') # convert to Unicode, which is u'\\xE4'\n",
    "print(utf_s)\n",
    "\n",
    "utf_s = s.decode('ascii', errors='replace') # convert to Unicode, which is u'\\xE4'\n",
    "print(utf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
