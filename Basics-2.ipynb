{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Assignment 2</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Define a function to analyze a numpy array (5 points)\n",
    " - Assume we have an array which contains term frequency of each document. Where each row is a document, each column is a word, and the value denotes the frequency of the word in the document. Define a function named \"analyze_tf\" which:\n",
    "      * has two input parameters: (1 point)\n",
    "        * a rank 2 input array\n",
    "        * a parameter \"binary\" with a default value set to False\n",
    "      * does the following steps in sequence:\n",
    "        1. if \"binary\" is True, binarizes the input array, i.e. if a value is greater than 1, change it to 1. (1 point)\n",
    "        2. normalizes the frequency of each word as: word frequency divided by the length of the document (i.e. sum of each row). Save the result as an array named **tf** (i.e. term frequency). The sum of each row of tf should be 1. (1 point)\n",
    "        3. calculates the document frequency (**df**) of each word, i.e. how many documents contain a specific word (0.5 point)\n",
    "        4. calculate the inverse document frequency (**idf**) of each word as ** N/df ** (df divided by N) where N is the number of documents (0.5 point)\n",
    "        5. calculates **tf_idf** array as: **tf * log (idf)** (tf multiply the log (base e) of idf ). The reason is, if a word appears in most documents, it does not have the discriminative power and often is called a \"stop\" word. The inverse of df can downgrade the weight of such words.(1 point)\n",
    "      * returns the tf_idf array.\n",
    " - Note, for all the steps, ** do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Define a function to analyze car dataset using pandas (5 points)\n",
    " - Define a function named \"analyze_cars\" to do the follows:\n",
    "   * Take a csv file path string as an input. Assume the csv file is in the format of the provided sample file.(0.5 point)\n",
    "   * Read the csv file as a dataframe with the first row as column names (0.5 point)\n",
    "   * Find cars with top 3 mpg among those of origin = 1. Print the names (i.e. \"car\" column) and mpg of these three cars. (1 point)\n",
    "   * Create a new column called \"brand\" to store the brand name as the first word in \"car\" column (hint: use \"apply\" function) (1 point)\n",
    "   * Show the mean, min, and max mpg values for each of these brands: \"ford\", \"buick\" and \"honda\"(1 point)\n",
    "   * Create a cross tab to show the average mpg of each brand and each origin value. Use \"brand\" as row index and \"origin\" as column index. (1 point)\n",
    " - This function does not have any return. Just print out the result of each calculation step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3 (Bonus). More sophisticated analyze_tf function (3 points)\n",
    " - Assume we have an array which contains term frequency of each document. Where each row is a document, each column is a word, and the value denotes the frequency of the word in the document. Define a function named \"advanced_analyze_tf\" which: \n",
    "      * has three input parameters: \n",
    "        * a rank 2 input array\n",
    "        * a parameter \"min_df\" (minimum document frequency) with a default value set to 0 \n",
    "        * a parameter \"max_words\" (maximum number of words) with a default value set None\n",
    "      * process the input array as follows in sequence:\n",
    "        1. if \"min_df\">0, remove words with document frequency (df) less than \"min_df\", i.e. the corresponding columns are removed (1 point)\n",
    "        2. if \"max_words\"> 0 and \"max_words\" < the total number of columns (M), only words with top \"max_words\" frequency (df) are kept. M - \"max_words\" columns are removed from the array (1 point)\n",
    "      * call the analyze_tf function in Q1 using the resulting array to get an tf_idf array (0.5 point)\n",
    "      * returns tf_idf and the original indexes of remaining words. (0.5 point)\n",
    " - Note, for all the steps, ** do not use any loop**. Just use array functions and broadcasting for high performance computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guideline##\n",
    "- Following the solution template provided below. Use __main__ block to test your functions\n",
    "- Save your code into a python file (e.g. assign2.py) that can be run in a python 3 environment. In Jupyter Notebook, you can export notebook as .py file in menu \"File->Download as\".\n",
    "- Make sure you have all import statements. To test your code, open a command window in your current python working folder, type \"python assign1.py\" to see if it can run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 2 0 1]\n",
      " [1 0 1 1 2 0]\n",
      " [0 0 2 0 0 1]\n",
      " [0 0 1 1 1 1]]\n",
      "\n",
      "Q1, binary=False [[0.         0.34657359 0.         0.14384104 0.         0.07192052]\n",
      " [0.27725887 0.         0.05753641 0.05753641 0.27725887 0.        ]\n",
      " [0.         0.         0.19178805 0.         0.         0.09589402]\n",
      " [0.         0.         0.07192052 0.07192052 0.1732868  0.07192052]]\n",
      "\n",
      "Q1, binary=True [[0.         0.46209812 0.         0.09589402 0.         0.09589402]\n",
      " [0.34657359 0.         0.07192052 0.07192052 0.1732868  0.        ]\n",
      " [0.         0.         0.14384104 0.         0.         0.14384104]\n",
      " [0.         0.         0.07192052 0.07192052 0.1732868  0.07192052]]\n",
      "\n",
      "Q2\n",
      "                     car   mpg\n",
      "194   chevrolet chevette  29.0\n",
      "82       dodge colt (sw)  28.0\n",
      "29   chevrolet vega 2300  28.0\n",
      "           mean  amin  amax\n",
      "brand                      \n",
      "buick  14.75000  12.0  21.0\n",
      "ford   17.12069  10.0  26.0\n",
      "honda  28.50000  24.0  33.0\n",
      "origin              1          2          3\n",
      "brand                                      \n",
      "amc         17.190476        NaN        NaN\n",
      "audi              NaN  24.000000        NaN\n",
      "bmw               NaN  26.000000        NaN\n",
      "buick       14.750000        NaN        NaN\n",
      "capri       25.000000        NaN        NaN\n",
      "chevrolet   17.814815        NaN        NaN\n",
      "chevy       10.000000        NaN        NaN\n",
      "chrysler    13.000000        NaN        NaN\n",
      "datsun            NaN        NaN  28.250000\n",
      "dodge       18.636364        NaN        NaN\n",
      "fiat              NaN  27.714286        NaN\n",
      "ford        17.120690        NaN        NaN\n",
      "honda             NaN        NaN  28.500000\n",
      "mazda             NaN        NaN  18.500000\n",
      "mercury     16.400000        NaN        NaN\n",
      "oldsmobile  11.666667        NaN        NaN\n",
      "opel              NaN  25.750000        NaN\n",
      "peugeot           NaN  24.750000        NaN\n",
      "plymouth    17.578947        NaN        NaN\n",
      "pontiac     16.125000        NaN        NaN\n",
      "renault           NaN  26.500000        NaN\n",
      "saab              NaN  24.666667        NaN\n",
      "subaru            NaN        NaN  26.000000\n",
      "toyota            NaN        NaN  25.833333\n",
      "volkswagen        NaN  25.500000        NaN\n",
      "volvo             NaN  19.666667        NaN\n",
      "\n",
      "Q3\n",
      "[[0.         0.34657359 0.         0.14384104 0.         0.07192052]\n",
      " [0.27725887 0.         0.05753641 0.05753641 0.27725887 0.        ]\n",
      " [0.         0.         0.19178805 0.         0.         0.09589402]\n",
      " [0.         0.         0.07192052 0.07192052 0.1732868  0.07192052]]\n",
      "[0 1 2 3 4 5]\n",
      "\n",
      "Q3, min_df=2 [[0.         0.19178805 0.         0.09589402]\n",
      " [0.07192052 0.07192052 0.34657359 0.        ]\n",
      " [0.19178805 0.         0.         0.09589402]\n",
      " [0.07192052 0.07192052 0.1732868  0.07192052]]\n",
      "[2 3 4 5]\n",
      "\n",
      "Q3, max_words=3\n",
      "[[0.         0.19178805 0.09589402]\n",
      " [0.14384104 0.14384104 0.        ]\n",
      " [0.19178805 0.         0.09589402]\n",
      " [0.09589402 0.09589402 0.09589402]]\n",
      "[2 3 5]\n"
     ]
    }
   ],
   "source": [
    "# Structure of your solution to Assignment 1 \n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def car_analysis(filepath):\n",
    "    \n",
    "    # read data\n",
    "    df=pd.read_csv(filepath,  header=0)\n",
    "    #print(df)\n",
    "    # sort\n",
    "    \n",
    "    print(df[df[\"origin\"]==1].sort_values(by=\"mpg\", ascending=False).iloc[0:3][[\"car\", \"mpg\"]])\n",
    "    \n",
    "    # get brand column\n",
    "    df['brand']=df.apply(lambda x: x[\"car\"].split(\" \")[0], axis=1)\n",
    "    \n",
    "    # get min, max, max\n",
    "    print(df[df[\"brand\"].isin([\"ford\",\"buick\", \"honda\"])].groupby(\"brand\")\\\n",
    "['mpg'].agg([np.mean, np.min, np.max]))\n",
    "\n",
    "    # get cross tab\n",
    "    print(pd.crosstab(columns=df.origin,index=df.brand, values=df.mpg, aggfunc=np.mean ))\n",
    "    # add your code\n",
    "\n",
    "\n",
    "def analyze_tf(arr, binary=False):\n",
    "    # suppose arr has shape (m,n)\n",
    "    \n",
    "    # binarize if binary=True\n",
    "    if binary:\n",
    "        arr=np.where(arr>0,1,0)\n",
    "    \n",
    "    # normalize, tf shape: (m,n)\n",
    "    # np.sum(arr, axis=1) has shape (m,)\n",
    "    # use [:,None] to make it (m,1) for broadcasting\n",
    "    tf=arr/(np.sum(arr, axis=1)[:,None])\n",
    "\n",
    "    # get df, shape (n,)\n",
    "    df=np.sum(np.where(arr>0, 1, 0), axis=0)\n",
    "\n",
    "    # get idf, shape (n,)\n",
    "    idf=arr.shape[0]/df\n",
    "    \n",
    "    # get tf_idf\n",
    "    tf_idf=tf*np.log(idf[None,:])  \n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "def advanced_analyze_tf(arr, binary=False, min_df=0, max_words=None):\n",
    "    # suppose arr has shape (m,n)\n",
    "    \n",
    "    # by default, all words are returned\n",
    "    selected_words=np.arange(arr.shape[1])\n",
    "    \n",
    "    df=np.sum(np.where(arr>0,1,0), axis=0)\n",
    "    \n",
    "    # process min_df\n",
    "    if min_df>0:\n",
    "        # get indexes of words with df>=min_df. \n",
    "        # min_df_selection is a one-dimension array\n",
    "        selected_words=np.where(df>=min_df)[0]\n",
    "        \n",
    "        # select columns from df\n",
    "        df=df[selected_words]\n",
    "        \n",
    "        \n",
    "    # process max_words\n",
    "    if max_words!=None:\n",
    "        if max_words>0 and max_words<arr.shape[1]:\n",
    "            \n",
    "            # sort df to get indexes of top max_words \n",
    "            # note that df may have been modified by min_df condition\n",
    "            # indexes returned are not the original word index\n",
    "           \n",
    "            max_words_selection = np.argsort(df)[-max_words:]\n",
    "            \n",
    "            # get original word indexes\n",
    "            selected_words=selected_words[max_words_selection]\n",
    "            \n",
    "            \n",
    "    arr=arr[:, selected_words]\n",
    "    \n",
    "    tf_idf=analyze_tf(arr, binary)\n",
    "    \n",
    "    return tf_idf, selected_words\n",
    "\n",
    "# best practice to test your class\n",
    "# if your script is exported as a module,\n",
    "# the following part is ignored\n",
    "# this is equivalent to main() in Java\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    arr=np.array([[0,1,0,2,0,1],[1,0,1,1,2,0],[0,0,2,0,0,1], [0,0,1,1,1,1]])\n",
    "    \n",
    "    print(arr)\n",
    "    \n",
    "    tf_idf=analyze_tf(arr)\n",
    "    print(\"\\nQ1, binary=False\",tf_idf)\n",
    "    \n",
    "    tf_idf=analyze_tf(arr, binary=True)\n",
    "    print(\"\\nQ1, binary=True\",tf_idf)\n",
    "    \n",
    "    # test question 2 \n",
    "    print(\"\\nQ2\")\n",
    "    car_analysis('../../dataset/cars.csv')\n",
    "    \n",
    "    # test question 3\n",
    "    tf_idf, selected_words=advanced_analyze_tf(arr)\n",
    "    print(\"\\nQ3\")\n",
    "    print(tf_idf)\n",
    "    print(selected_words)\n",
    "    \n",
    "    tf_idf, selected_words=advanced_analyze_tf(arr, min_df=2)\n",
    "    print(\"\\nQ3, min_df=2\", tf_idf)\n",
    "    print(selected_words)\n",
    "    \n",
    "    tf_idf, selected_words=advanced_analyze_tf(arr, max_words=3)\n",
    "    print(\"\\nQ3, max_words=3\")\n",
    "    print(tf_idf)\n",
    "    print(selected_words)\n",
    "    \n",
    "   # tf_idf, selected_words=advanced_analyze_tf(arr, min_df=1, max_words=3)\n",
    "   # print(\"\\nQ3, min_df=2, max_words=3\")\n",
    "   # print(tf_idf)\n",
    "   # print(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
